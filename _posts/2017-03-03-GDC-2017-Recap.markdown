---
layout: 	post
title:  	"GDC 2017 Recap"
date:   	2017-03-03 21:57:46 -0500
category: 	"Conference"
published:	true
---

I was lucky enough to be able to visit GDC this year. As always, there were a lot of interesting talks, ideas and announcements to come out of it. I wasn't able to go to each one of these, but wanted to do a quick info dump of the ones I was able to attend. Quick disclaimer that this post isn't meant to be comprehensive (I'm only covering rendering/GPU/engine talks and I might have also very well missed something, in which case please point me to it), and it might also be subjective to an extent (I might be wrong, in which case please correct me :)). Also, Krzysztof Narkowicz is collating a list of presentation slides which you can find on his blog [here](https://knarkowicz.wordpress.com/2017/03/01/gdc-2017-presentations/). I'll will update this article with links to the presentations as and when they become available.

### Day 1

The first day of the conference was Low-level API day with D3D12 and Vulkan being the main focus. The talks included.

* **"D3D12 and Vulkan Done Right" & "Wave Programming in D3D12 and Vulkan** - Holger Gruen(NVIDIA), David Lively (AMD), Gareth Thomas (AMD)
* **How to Thrive on the Bleeding Edge Whilst Avoiding Death by 1,000 Paper Cuts** - Jurjen Katsman (Nixxes)
* **"Async Compute: Deep Dive" & "Raster Ordered Views and Conservative Rasterization** - Alex Dunn (NVIDIA), Stephan Hodes (AMD), Evgeny Makarov (NVIDIA), Rahul Sathe (NVIDIA)
* **Moving to DirectX 12: Lessons Learned** - Tiago Rodrigues (Ubisoft Montreal)
* **"Cinematic Depth of Field" & "Advanced Particle Simulation in Compute"** - Richard Tonge (NVIDIA), Hammad Mazhar (NVIDIA), Karl Hillesland (AMD)

The common theme was how to do resource barriers correctly, inspect and avoid memory overcommitment, and how to best deal with pipelines states (PSOs). Yuriy O'Donnel had an excellect talk on how to perform resource transitions and manage residency programmatically in Frostibe using FRAMEGRAPH. 

[Wave Programming](https://msdn.microsoft.com/en-us/library/windows/desktop/mt733232(v=vs.85).aspx) in SM6 "enable the elimination of barrier constructs when the scope of synchronization is within the width of the SIMD processor, or some other set of threads that are known to be atomic relative to each other." This allows for fast *parallel reduction*. Wave programming also allows one to *query* Helper Lanes (used for pixel quad gradients) which was previously abstracted away. Using *wave voting* schemes such as `WaveAllTrue()` allows one to choose optimal code path for divergent vs. non-divergent control flow.

Early adopters of DX12 explained some of the growing pains with it, although the situation seems to be improving with Microsoft releasing [PIX support for DX12 (beta)](https://blogs.msdn.microsoft.com/pix/2017/01/17/introducing-pix-on-windows-beta/) and post-[TDR](http://http.developer.nvidia.com/NsightVisualStudio/2.2/Documentation/UserGuide/HTML/Content/Timeout_Detection_Recovery.htm) analysis tools such as [NVIDIA Aftermath](https://developer.nvidia.com/nvidia-aftermath). It was also suggested that the driver (and complexity) cannot be overlooked even when working with low level APIs. At any rate, it will be hard for your first DX12 implementation to beat years of DX11 driver development and optimization. The best benefits of DX12 come from multi-GPU and async compute. As such, it was suggested that, it might be best to lose the DX11 training wheels and develop directly on DX12 so as to not constantly have to justify the DX12 time ivestment compared to DX11. 

For Async Compute, it is important to ensure proper work pairing (using fences). One needs to be aware of *async tax* when running workloads in this mode. It is best to maintain non-async code paths and profile rather than just fire-and-forget. Since async compute is like CPU HyperThreading (multiple threads sharing same H/W resource), it is important to monitor resource contention. Other than monitoring register pressure, one also needs to monitor cache trashing. One option suggested was to use dummy LDS to reduce occupancy so as to reduce cache trashing ¯\\_(ツ)\_/¯ Personally, I would love to see that information exposed to profiling tools.

Rahul Sathe (NVIDIA) talked about using programmable sample locations and conservative raterization. Using conservative rasterization violates the uniqueness of the [top-left rule](https://msdn.microsoft.com/en-us/library/windows/desktop/cc627092(v=vs.85).aspx) which "ensures that adjacent triangles are drawn once". This can cause flickering artifacts as writes for overlapping pixel shader invocations is unordered (“overlapping” is defined as invocations that are generated by the same draw calls and share the same pixel coordinate when in pixel-frequency execution mode, and the same pixel and sample coordinate in sample-frequency mode.) [Raster Order Views](https://msdn.microsoft.com/en-us/library/windows/desktop/dn914601(v=vs.85).aspx) are like UAVs but they ensure that pixel shader invocations are executed in the order in which the geometry is submitted. Pairing ROVs with conservative rasterization fixes the temporal instability.

The *Cinematic Depth of Field* talk proposed using a scatter-based approach for constant time variable width blur called [**Fast Filter Spreading**](https://www2.eecs.berkeley.edu/Pubs/TechRpts/2009/EECS-2009-54.pdf) which is like reverse-[Summed Area Tables](https://en.wikipedia.org/wiki/Summed_area_table) in spirit. Unlike SATs this can be extended to perform a Bartlett filter (tent filter) rather than a simple box filter. Also, unlike SATs which have a precision meltdown at higher resolutions, this technique is resolution independent and has a fixed cost, although that cost is quite high.











